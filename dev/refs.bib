@book{yuksel,
	author = {Serdar Yüksel and Tamer Başar},
	title = {Stochastic Networked Control Systems},
	publisher = {Birkhäuser New York, NY},
	edition = {2},
	year = {2013}
}

@article{cell-free-mimo,
	doi = {10.1561/2000000109},
	url = {https://doi.org/10.1561%2F2000000109},
	year = 2021,
	publisher = {Now Publishers},
	volume = {14},
	number = {3-4},
	pages = {162--472},
	author = {Özlem Tugfe Demir and Emil Björnson and Luca Sanguinetti},
	title = {Foundations of User-Centric Cell-Free Massive {MIMO}},
	journal = {Foundations and Trends{\textregistered} in Signal Processing}
}

@inproceedings{team-precoding,
	doi = {10.1109/ieeeconf53345.2021.9723321},
	url = {https://doi.org/10.1109%2Fieeeconf53345.2021.9723321},
	year = 2021,
	month = {oct},
	publisher = {{IEEE}},
	author = {Lorenzo Miretti and Emil Bjornson and David Gesbert},
	title = {Team Precoding Towards Scalable Cell-free Massive {MIMO} Networks},
	booktitle = {2021 55th Asilomar Conference on Signals, Systems, and Computers}
} 

@article{miretti2021team,
      title={Team MMSE Precoding with Applications to Cell-free Massive MIMO}, 
      author={Lorenzo Miretti and Emil Björnson and David Gesbert},
      year={2021},
      journal={arXiv preprint 2104.15027}
}

@article{miretti2022joint,
      title={Joint optimal beamforming and power control in cell-free massive MIMO}, 
      author={Lorenzo Miretti and Renato Luis Garrido Cavalcante and Slawomir Stanczak},
      year={2022},
      journal={arXiv preprint 2208.01385}
}

@book{folland2013real,
      title={Real Analysis: Modern Techniques and Their Applications},
      author={Folland, G.B.},
      isbn={9781118626399},
      series={Pure and Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts},
      url={https://books.google.de/books?id=wI4fAwAAQBAJ},
      year={2013},
      publisher={Wiley}
}

@article{vershynin2011introduction,
      title={Introduction to the non-asymptotic analysis of random matrices}, 
      author={Roman Vershynin},
      year={2011},
      journal={arXiv preprint 1011.3027}
}

@book{bauer,
	url = {https://doi.org/10.1515/9783110814668},
	title = {Probability Theory},
	author = {Heinz Bauer},
	publisher = {De Gruyter},
	address = {Berlin, New York},
	doi = {doi:10.1515/9783110814668},
	isbn = {9783110814668},
	year = {1996},
	lastchecked = {2023-07-18}
}

@book{berlinetKernels,
      title = {Reproducing Kernel Hilbert Spaces in Probability and Statistics},
      author = {Alain Berlinet and Christine Thomas-Agnan},
      year = {2004},
      publisher = {Springer},
      address = {New York, NY},
      doi = {10.1007/978-1-4419-9096-9},
      isbn = {978-1-4020-7679-4 (Hardcover)},
      note = {Published: 31 December 2004},
      series = {Springer Book Archive},
      copyright = {Springer Science+Business Media Dordrecht 2004},
      url = {https://doi.org/10.1007/978-1-4419-9096-9},
      edition = {1},
      pages = {XXII, 355},
      keywords = {Economics, general, Statistics for Business, Management, Economics, Finance, Insurance, Economic Theory/Quantitative Economics/Mathematical Methods},
}

@article{mollenhauer2023nonparametric,
      title={Nonparametric approximation of conditional expectation operators}, 
      author={Mattes Mollenhauer and Péter Koltai},
      year={2023},
      journal={arXiv preprint 2012.12917}
}

@article{gaussianKernel,
  title={Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory},
  author={Minh, H.Q.},
  journal={Constructive Approximation},
  volume={32},
  pages={307--338},
  year={2010},
  doi={10.1007/s00365-009-9080-0},
  url={https://doi.org/10.1007/s00365-009-9080-0}
}

@book{dudley2002real,
  title={Real Analysis and Probability},
  author={Dudley, R.},
  edition={2nd},
  year={2002},
  publisher={Cambridge University Press},
  series={Cambridge Studies in Advanced Mathematics},
  doi={10.1017/CBO9780511755347}
}

@book{bauschke_combettes_2011,
    author = {Heinz H. Bauschke and Patrick L. Combettes},
    title = {Convex Analysis and Monotone Operator Theory in Hilbert Spaces},
    series = {CMS Books in Mathematics},
    year = {2011},
    publisher = {Springer New York, NY},
    doi = {10.1007/978-1-4419-9467-7},
    edition = {1},
    pages = {XVI, 468},
    topics = {Calculus of Variations and Optimal Control; Optimization, Algorithms, Visualization},
    isbn = {978-1-4419-9466-0},
    copyright = {Springer Science+Business Media, LLC 2011},
    seriesissn = {1613-5237},
    serieseissn = {2197-4152},
}

@article{Klebanov_2021,
	doi = {10.3150/20-bej1308},
	url = {https://doi.org/10.3150%2F20-bej1308},
	year = 2021,
	month = {nov},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	volume = {27},
	number = {4},
	author = {Ilja Klebanov and Björn Sprungk and T.J. Sullivan},
	title = {The linear conditional expectation in Hilbert space},
	journal = {Bernoulli}
}

@misc{lalley2018conditional,
  author       = {Steven P. Lalley},
  title        = {Conditional Expectation},
  howpublished = {Chapter in lecture notes of \emph{Statistics 383: Measure Theoretic Probability 2} at the University of Chicago},
  year         = {2018},
}

@article{Smale2007,
  author    = {Steve Smale and Ding-Xuan Zhou},
  title     = {Learning Theory Estimates via Integral Operators and Their Approximations},
  journal   = {Constructive Approximation},
  year      = {2007},
  volume    = {26},
  number    = {2},
  pages     = {153--172},
  month     = {August},
  doi       = {10.1007/s00365-006-0659-y},
  issn      = {1432-0940},
  abstract  = {The regression problem in learning theory is investigated with least square Tikhonov regularization schemes in reproducing kernel Hilbert spaces (RKHS). We follow our previous work and apply the sampling operator to the error analysis in both the RKHS norm and the L2 norm. The tool for estimating the sample error is a Bennett inequality for random variables with values in Hilbert spaces. By taking the Hilbert space to be the one consisting of Hilbert-Schmidt operators in the RKHS, we improve the error bounds in the L2 metric, motivated by an idea of Caponnetto and de Vito. The error bounds we derive in the RKHS norm, together with a Tsybakov function we discuss here, yield interesting applications to the error analysis of the (binary) classification problem, since the RKHS metric controls the one for the uniform convergence.},
  url       = {https://doi.org/10.1007/s00365-006-0659-y},
}

@article{vRKHS,
  author    = {Micchelli, Charles A. and Pontil, Massimiliano},
  title     = {On Learning Vector-Valued Functions},
  journal   = {Neural Computation},
  year      = {2005},
  volume    = {17},
  number    = {1},
  pages     = {177--204},
  month     = {January},
  doi       = {10.1162/0899766052530802},
  pmid      = {15563752},
}

@article{artacho2019douglasrachford,
      title={The {D}ouglas-{R}achford Algorithm for Convex and Nonconvex Feasibility Problems}, 
      author={Francisco J. Aragón Artacho and Rubén Campoy and Matthew K. Tam},
      year={2019},
      journal={arXiv Preprint 1904.09148},
}

@phdthesis{lorenzoThesis,
  author = {Miretti, Lorenzo},
  title = {Optimal designs for decentralized transmission with asymmetric channel state information},
  year = {2021},
  note = {© EURECOM. Personal use of this material is permitted. The definitive version of this paper was published in Thesis and is available at :},
    school = {EURECOM}
}

@book{shalev-shwartz_ben-david_2014, 
    place={Cambridge}, 
    title={Understanding Machine Learning: From Theory to Algorithms}, 
    DOI={10.1017/CBO9781107298019}, 
    publisher={Cambridge University Press}, 
    author={Shalev-Shwartz, Shai and Ben-David, Shai}, 
    year={2014}
}

@article{Klebanov_2020,
	doi = {10.1137/19m1305069},
	url = {https://doi.org/10.1137%2F19m1305069},
	year = 2020,
	month = {jan},
	publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
	volume = {2},
	number = {3},
	pages = {583--606},
	author = {Ilja Klebanov and Ingmar Schuster and T. J. Sullivan},
	title = {A Rigorous Theory of Conditional Mean Embeddings},
	journal = {{SIAM} Journal on Mathematics of Data Science}
}

@InProceedings{smola-kernel-embedding,
    author="Smola, Alex and Gretton, Arthur and Song, Le and Sch{\"o}lkopf, Bernhard",
    editor="Hutter, Marcus
    and Servedio, Rocco A.
    and Takimoto, Eiji",
    title="A {H}ilbert Space Embedding for Distributions",
    booktitle="Algorithmic Learning Theory",
    year="2007",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="13--31",
    abstract="We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.",
    isbn="978-3-540-75225-7"
}

@book{gockenbach2016linear,
  title={Linear Inverse Problems and {T}ikhonov Regularization},
  author={Gockenbach, M.S.},
  isbn={9780883851418},
  lccn={2016935233},
  series={The Carus Mathematical Monographs},
  url={https://books.google.de/books?id=FhuzDwAAQBAJ},
  year={2016},
  publisher={Mathematical Association of America}
}

@book{gaussianProcesses,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    title = "{Gaussian Processes for Machine Learning}",
    publisher = {The MIT Press},
    year = {2005},
    month = {11},
    abstract = "{A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.}",
    isbn = {9780262256834},
    doi = {10.7551/mitpress/3206.001.0001},
    url = {https://doi.org/10.7551/mitpress/3206.001.0001},
}

@article{manton2015primer,
    title={A Primer on Reproducing Kernel {H}ilbert Spaces}, 
    author={Jonathan H. Manton and Pierre-Olivier Amblard},
    year={2015},
    journal={arXiv preprint 1408.0952}
}


@InProceedings{representerTheorem,
    author="Sch{\"o}lkopf, Bernhard
    and Herbrich, Ralf
    and Smola, Alex J.",
    editor="Helmbold, David
    and Williamson, Bob",
    title="A Generalized Representer Theorem",
    booktitle="Computational Learning Theory",
    year="2001",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="416--426",
    abstract="Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.",
    isbn="978-3-540-44581-4"
}

@article{carmeli2008vector,
      title={Vector valued reproducing kernel {H}ilbert spaces and universality}, 
      author={C. Carmeli and E. De Vito and A. Toigo and V. Umanità},
      year={2008},
      journal={arXiv preprint 0807.1659}
}

@article{gauss-seidel,
    author = {de Pillis, John},
    title = {Gauss–Seidel Convergence for Operators on Hilbert Space},
    journal = {SIAM Journal on Numerical Analysis},
    volume = {10},
    number = {1},
    pages = {112-122},
    year = {1973},
    doi = {10.1137/0710012},
    URL = { https://doi.org/10.1137/0710012},
    eprint = { https://doi.org/10.1137/0710012},
    abstract = { For invertible linear operator A on Hilbert space \$\mathcal{H}\$, we consider classes of splittings for A (viz., \$A = A\_1 + A\_2 \$, where \$A\_1 ,A\_2 \$ and \$A\_1^{ - 1} \$ exist as a bounded linear operator on \$\mathcal{H}\$) for which \$\rho (A\_1^{ - 1} A\_2 )\$, the spectral radius of \$A\_1^{ - 1} A\_2 \$, is less than one. For example, we introduce the class of \$\alpha \$-splittings (Definition 3.2) which generalizes the classical notion of the lower-triangular \$A\_1 \$ and upper-triangular \$A\_2 \$ splitting for an \$n \times n\$ matrix A. Tests are presented which guarantee Gauss–Seidel convergence (i.e., \$\rho (A\_1^{ - 1} A\_2 ) < 1\$) relative to \$\alpha \$-splittings (cf. Theorem 3.6, Corollary 3.7, Corollary 3.8). Another class of splittings for (self-adjoint) linear operators \$A = A\_1 + A\_2 \$ is an operator-parameter generalization of the \$\omega \$-splitting which occurs in the matrix method of successive overrelaxation. In fact, Theorem 4.2, which establishes conditions on this splitting sufficient for Gauss–Seidel convergence, has, as a consequence, the theorems of Ostrowski (Corollary 4.3) and, hence, of Reich (cf. Example, § 4). }
}

@book{werner2018funktionalanalysis,
  title = {Funktionalanalysis},
  author = {Dirk Werner},
  series = {Springer-Lehrbuch},
  doi = {10.1007/978-3-662-55407-4},
  publisher = {Springer Spektrum},
  address = {Berlin, Heidelberg},
  year = {2018},
  edition = {8},
  isbn_softcover = {978-3-662-55406-7},
  isbn_ebook = {978-3-662-55407-4},
  pages = {XIII, 586},
  illustrations = {21 b/w illustrations},
  topics = {Functional Analysis},
}

@article{radner,
author = {R. Radner},
title = {{Team Decision Problems}},
volume = {33},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {857 -- 881},
year = {1962},
doi = {10.1214/aoms/1177704455},
URL = {https://doi.org/10.1214/aoms/1177704455}
}

@article{teams-convex-analysis,
author = {Y\"{u}ksel, Serdar and Saldi, Naci},
title = {Convex Analysis in Decentralized Stochastic Control, Strategic Measures, and Optimal Solutions},
journal = {SIAM Journal on Control and Optimization},
volume = {55},
number = {1},
pages = {1-28},
year = {2017},
doi = {10.1137/15M1049129},
URL = {https://doi.org/10.1137/15M1049129},
eprint = {https://doi.org/10.1137/15M1049129},
    abstract = { This paper is concerned with the properties of the sets of strategic measures induced by admissible team policies in decentralized stochastic control and the convexity properties in dynamic team problems. To facilitate a convex analytical approach, strategic measures for team problems are introduced. Properties such as convexity, compactness, and Borel measurability under weak convergence topology are studied, and sufficient conditions for each of these properties are presented. These lead to existence of and structural results for optimal policies. It will be shown that the set of strategic measures for teams which are not classical is in general nonconvex, but the extreme points of a relaxed set consist of deterministic team policies, which lead to their optimality for a given team problem under an expected cost criterion. Externally provided independent common randomness for static teams or private randomness for dynamic teams do not improve the team performance. The problem of when a sequential team problem is convex is studied and necessary and sufficient conditions for problems which include teams with a nonclassical information structure are presented. Implications of this analysis in identifying probability and information structure dependent convexity properties are presented. }
}

@book{golub1983matrix,
  title={Matrix Computations},
  author={Golub, G.H. and Van Loan, C.F.},
  isbn={9780801830105},
  lccn={83007897},
  series={Johns Hopkins Studies in Atlantic History \& Culture},
  url={https://books.google.de/books?id=g4gQAQAAIAAJ},
  year={1983},
  publisher={Johns Hopkins University Press}
}

@misc{julia,
  title = {The Julia Language},
  author = {{The Julia Language contributors}},
  howpublished = {\url{https://julialang.org/}},
  version = {1.9.2},
  note = {Accessed: 23.11.2023}
}

@misc{hu2023recent,
      title={Recent Developments in Machine Learning Methods for Stochastic Control and Games}, 
      author={Ruimeng Hu and Mathieu Laurière},
      year={2023},
      eprint={2303.10257},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{han2016deep,
      title={Deep Learning Approximation for Stochastic Control Problems}, 
      author={Jiequn Han and Weinan E},
      year={2016},
      eprint={1611.07422},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}