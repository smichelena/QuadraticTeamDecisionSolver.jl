var documenterSearchIndex = {"docs":
[{"location":"quadTeamProblems/#General-Types-and-Utility-Functions","page":"General Static Quadratic Team Problem Functions","title":"General Types and Utility Functions","text":"","category":"section"},{"location":"quadTeamProblems/","page":"General Static Quadratic Team Problem Functions","title":"General Static Quadratic Team Problem Functions","text":"These types and functions provide basic tools for defining and dealing with static quadratic team problems.","category":"page"},{"location":"quadTeamProblems/#Types","page":"General Static Quadratic Team Problem Functions","title":"Types","text":"","category":"section"},{"location":"quadTeamProblems/","page":"General Static Quadratic Team Problem Functions","title":"General Static Quadratic Team Problem Functions","text":"QuadTeamProblem{T<:Number}","category":"page"},{"location":"quadTeamProblems/#QuadraticTeamDecisionSolver.QuadTeamProblem","page":"General Static Quadratic Team Problem Functions","title":"QuadraticTeamDecisionSolver.QuadTeamProblem","text":"QuadTeamProblem{T <: Number}\n\nThe QuadTeamProblem struct represents a quadratic team decision problem. In essence, it stores the correct problem dimensions and the field over which the problem is solved.\n\nFields\n\nN::Int: Number of agents in the team.\nm::Vector{Int}: Array of measurement dimensions for each agent.\na::Vector{Int}: Array of action space dimensions for each agent.\nT::Type{T}: Numeric type for the problem.\n\n\n\n\n\n","category":"type"},{"location":"quadTeamProblems/#Functions","page":"General Static Quadratic Team Problem Functions","title":"Functions","text":"","category":"section"},{"location":"quadTeamProblems/","page":"General Static Quadratic Team Problem Functions","title":"General Static Quadratic Team Problem Functions","text":"checkProblem(p::QuadTeamProblem)\ncheckGamma(p::QuadTeamProblem, γ::Vector{<:Function})\nresidual(\n  m::Int,\n  p::QuadTeamProblem,\n  kernels::Vector{<:Function},\n  γ::Vector{<:Vector{<:Vector}},\n  Y::Vector{<:Vector},\n  Q::Matrix{<:Vector},\n  R::Vector{<:Vector},\n  λ::Vector{<:AbstractFloat},\n )\ngammaNorm(f::Function, Y::AbstractVector)\nGammaNorm(F::Vector{<:Function}, Y::AbstractVector)","category":"page"},{"location":"quadTeamProblems/#QuadraticTeamDecisionSolver.checkProblem-Tuple{QuadTeamProblem}","page":"General Static Quadratic Team Problem Functions","title":"QuadraticTeamDecisionSolver.checkProblem","text":"checkProblem(p::QuadTeamProblem)\n\nCheck the consistency and correctness of a QuadTeamProblem object p.\n\nArguments\n\np::QuadTeamProblem: The QuadTeamProblem object representing the problem specification.\n\nErrors\n\nAssertionError: Throws an error if the problem specification is incorrect or inconsistent.\n\nExample\n\nP = QuadTeamProblem(...)\ncheckProblem(P)\n\n\n\n\n\n","category":"method"},{"location":"quadTeamProblems/#QuadraticTeamDecisionSolver.checkGamma-Tuple{QuadTeamProblem, Vector{<:Function}}","page":"General Static Quadratic Team Problem Functions","title":"QuadraticTeamDecisionSolver.checkGamma","text":"checkGamma(p::QuadTeamProblem, γ::Vector{<:Function})\n\nCheck the output dimensions of the functions γ for each agent in the QuadTeamProblem P.\n\nArguments\n\nP::QuadTeamProblem: The QuadTeamProblem specifying the problem.\nγ::Vector{Function}: Vector of functions γ for each agent.\n\nReturns\n\nγ::Vector{Function}: The input vector of functions γ.\n\nErrors\n\nThrows an error if the output dimensions of the functions γ do not match the specified dimensions in P.\n\n\n\n\n\n","category":"method"},{"location":"quadTeamProblems/#QuadraticTeamDecisionSolver.residual-Tuple{Int64, QuadTeamProblem, Vector{<:Function}, Vector{<:Vector{<:Vector}}, Vector{<:Vector}, Matrix{<:Vector}, Vector{<:Vector}, Vector{<:AbstractFloat}}","page":"General Static Quadratic Team Problem Functions","title":"QuadraticTeamDecisionSolver.residual","text":"residual(\n\tm::Int,\n\tp::QuadTeamProblem,\n\tkernels::Vector{<:Function},\n\tγ::Vector{<:Vector{<:Vector}},\n\tY::Vector{<:Vector},\n\tQ::Matrix{<:Vector},\n\tR::Vector{<:Vector},\n\tλ::Vector{<:AbstractFloat},\n)\n\nCompute the residual function\n\nmathrmres(gamma) = mathbfAgamma + mathbftildeR\n\nfor a given tuple of policies gamma = (gamma^1 dots gamma^N).\n\nArguments\n\nm::Int: Number of training samples.\np::QuadTeamProblem: A quad team problem object.\nkernels::Vector{<:Function}: Vector of kernel functions.\nγ::Vector{<:Vector{<:Vector}}: Vector of gamma kernel function coefficients.\nY::Vector{<:Vector}: Vector of measurement vector samples.\nQ::Matrix{<:Vector}: Matrix of system matrix samples, organized block-wise.\nR::Vector{<:Vector}: Vector of linear term samples, organized block-wise.\n\nReturns\n\nres: vector of kernel function coefficient vectors corresponding to residual function.\n\n\n\n\n\n","category":"method"},{"location":"quadTeamProblems/#QuadraticTeamDecisionSolver.gammaNorm-Tuple{Function, AbstractVector}","page":"General Static Quadratic Team Problem Functions","title":"QuadraticTeamDecisionSolver.gammaNorm","text":"gammaNorm(f::Function, Y::AbstractVector)\n\nCompute the Gamma^i norm of a given function f in Gamma^i approximated  using measurement vector data, that is, samples of mathbfY_i.\n\nArguments\n\nf::Function: The function for which the norm is to be computed.\nY::AbstractVector: Input data vector.\n\nReturns\n\nnorm: Gamma^i-norm of f.\n\nDetails\n\nThe gamma norm is calculated as the squared Euclidean norm of the function's evaluations on the input data, normalized by the length of the input data.\n\n\n\n\n\n","category":"method"},{"location":"quadTeamProblems/#QuadraticTeamDecisionSolver.GammaNorm-Tuple{Vector{<:Function}, AbstractVector}","page":"General Static Quadratic Team Problem Functions","title":"QuadraticTeamDecisionSolver.GammaNorm","text":"GammaNorm(F::Vector{<:Function}, Y::AbstractVector)\n\nCompute the Gamma norm of a given function tuple F = (f_1 dots f_i) in Gamma = Gamma^itimesdotstimesGamma^N  approximated using measurement vector data, that is, samples of mathbfY = (mathbfY_idotsmathbfY_N).\n\nArguments\n\nF::Vector{<:Function}: Vector of functions for which the gamma norm is to be computed.\nY::AbstractVector: Input data vector.\n\nReturns\n\nnorm: Gamma-norm of F.\n\n\n\n\n\n","category":"method"},{"location":"teamMMSE/#Team-Mininum-Mean-Square-Error-Precoding","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"","category":"section"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"Here we provide a simple interface and sample generation tool for producing a toy simulation that we can test our solvers on.","category":"page"},{"location":"teamMMSE/#General-Team-Minimum-Mean-Square-Error-Problem-Description","page":"Team Mininum Mean Square Error Precoding","title":"General Team Minimum Mean Square Error Problem Description","text":"","category":"section"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"The team MMSE precoding problem is in general a static quadratic team decision problem. In particular, if we have N transmitters each equipped with L antennas and K single antenna receivers, then for every k in 1dots K, the team MMSE problem reads","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"argmin_gamma_k in Gamma^1 times dots times Gamma^N mathrmMSE_k(gamma) = mathbbE left leftlVert mathbfH^mathsfHgamma_k(mathbfY) - mathbfe_k rightrVert^2 + frac1PleftlVert gamma_k(mathbfY) rightrVert^2 right","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"The random variable","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"mathbfH = mathbfH_1 dots mathbfH_N","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"is called the channel and is centrally symmetric complex gaussian with realizations in mathbbC^K LN and arbitrary covariance.","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"For each transmitter (agent) i in 1dots N , the measurement random vector mathbfY_i is given by","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"mathbfY_i = mathbfH + mathbfZ_i","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"where mathbfZ_i is the channel estimation error at transmitter (agent) i and is also centrally symmetric complex gaussian with realizations in ``\\mathbb{C}^{K,LN} and arbitrary covariance.","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"For further details on team MMSE precoding, please refer to (Miretti et al., oct 2021) and (Miretti et al., 2022).","category":"page"},{"location":"teamMMSE/#Our-Situation","page":"Team Mininum Mean Square Error Precoding","title":"Our Situation","text":"","category":"section"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"The previously defined measurement model is quite general. While the solvers we provide should be able to handle all measurement schemes that can be constructed from this model in this library, we implement the following model:","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"mathbfY_i = beginbmatrix dots  varepsilonmathbfH_i-1 + (1 - varepsilon)mathbfN_i-1  mathbfH_i  varepsilonmathbfH_i+1 + (1 - varepsilon)mathbfN_i+1  dots endbmatrix","category":"page"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"where mathbfH_i is the i-th block column of the channel mathbfH and mathbfN is centrally symmetric gaussian noise with diagonal covariance matrix.","category":"page"},{"location":"teamMMSE/#Types","page":"Team Mininum Mean Square Error Precoding","title":"Types","text":"","category":"section"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"teamMMSEproblem","category":"page"},{"location":"teamMMSE/#QuadraticTeamDecisionSolver.teamMMSEproblem","page":"Team Mininum Mean Square Error Precoding","title":"QuadraticTeamDecisionSolver.teamMMSEproblem","text":"struct teamMMSEproblem\n\nA struct to specify parameters for a problem from the team MMSE class.\n\nAttributes:\n\nP::AbstractFloat: The Signal-to-Interference-plus-Noise Ratio (SINR) for the problem.\nN::Int: The number of transmitters in the problem.\nL::Int: The number of antennas at each transmitter.\nK::Int: The number of receivers in the problem.\nσₕ::Vector{AbstractFloat}: Vector specifying channel covariances.\nσₙ::Vector{AbstractFloat}: Vector specifying noise covariances.\nϵ::Vector{AbstractFloat}: vector of link coefficients varepsilon_i in (01) i in 1dotsN.\n\n\n\n\n\n","category":"type"},{"location":"teamMMSE/#Functions","page":"Team Mininum Mean Square Error Precoding","title":"Functions","text":"","category":"section"},{"location":"teamMMSE/","page":"Team Mininum Mean Square Error Precoding","title":"Team Mininum Mean Square Error Precoding","text":"sampleComplexNormal(σ::AbstractFloat, K::Int, L::Int)\ngenerateTeamMMSEsamples(t::teamMMSEproblem, m::Int)","category":"page"},{"location":"teamMMSE/#QuadraticTeamDecisionSolver.sampleComplexNormal-Tuple{AbstractFloat, Int64, Int64}","page":"Team Mininum Mean Square Error Precoding","title":"QuadraticTeamDecisionSolver.sampleComplexNormal","text":"sampleComplexNormal(σ::AbstractFloat, K::Int, L::Int)\n\nSample from the circularly symmetric complex Gaussian distribution.\n\nParameters:\n\nσ::AbstractFloat: The standard deviation of the normal distributions for both the real and imaginary parts of the complex numbers.\nK::Int: The number of rows in the output matrix.\nL::Int: The number of antennas\n\nReturns:\n\nA KxN matrix of complex numbers with circularly symmetric Gaussian distribution.\n\n\n\n\n\n","category":"method"},{"location":"teamMMSE/#QuadraticTeamDecisionSolver.generateTeamMMSEsamples-Tuple{teamMMSEproblem, Int64}","page":"Team Mininum Mean Square Error Precoding","title":"QuadraticTeamDecisionSolver.generateTeamMMSEsamples","text":"generateTeamMMSEsamples(t::teamMMSEproblem, m::Int)\n\ngenerate m samples for a team MMSE problem specified by t.\n\nArguments\n\nt::teamMMSEproblem: a teamMMSEproblem object specifying the statistics and sizes of channel and noise measurements \nm::Int: the desired amount of samples\n\nReturns\n\nH:: A vector of size t.N of vectors of size m of K times L channel measurement matrices\nY:: A vector of size t.N of vectors of size m of K times L times N agent measurement vectors\nQ:: A matrix of size t.N by t.N of vectors of size m of L times L matrices\nR:: A vector of size t.N of vectors of size m of L times 1 matrices\n\n\n\n\n\n","category":"method"},{"location":"fixedPointSolvers/#Fixed-Point-Solvers","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"","category":"section"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"Methods and Interface for building solvers for the quadratic team decision problem using empirical methods.","category":"page"},{"location":"fixedPointSolvers/#Explanation","page":"Fixed Point Solvers","title":"Explanation","text":"","category":"section"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"The solution to the static quadratic team decision problem is entirely characterized by a set of linear operator equations over the (product) policy space. These are called the Stationarity Equations, and for all i in  1 dots N they read","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"mathbbE left mathbfQ_ii vert mathbfY_i rightgamma^i(mathbfY_i) + sum_ij=1   j neq i mathbbE left mathbfQ_ij  gamma_(k)^j left( mathbfY_j right) vert mathbfY_i right + mathbbE left mathbfR_i vert mathbfY_i right = 0 text as","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"We can write these compactly as","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"mathbfAgamma = - tildemathbfR","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"Write","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"mathbfA = mathbfA_1 + mathbfA_2","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"with easily invertible mathbfA_1, and consider the iteration","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"gamma_k+1 = -mathbfA_1^-1left( mathbfA_2gamma_k + tildemathbfR right)","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"which is a fixed point iteration. This iteration is generated by the operator splitting mathbfA = mathbfA_1 + mathbfA_2. For this reason, we dub the methods implemented in this library operator splitting methods, which, given our problem structure, are entirely identical to their finite dimensional matric analogues.","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"For more details on this class of operator splittings, please see (de Pillis, 1973).","category":"page"},{"location":"fixedPointSolvers/#Convergence-Behavior","page":"Fixed Point Solvers","title":"Convergence Behavior","text":"","category":"section"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"The convergence behavior of the methods implemented in this library depends upon the eigenvalues of the system matrix mathbfQ. In particular, the assumption that mathbfQ is bounded away from zero implies that the decomposition","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"mathbfQ =  mathbfT + lambda_mathrmminI","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"for positive semidefinite mathbfT is always valid.","category":"page"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"The convergence of our schemes then depends upon the quantitoesvarrho(mathbfT) and lambda_mathrmmin. Note that varrho(mathbfT) exists and is finite as, by assumption, mathbfQ is uniformly bounded above.","category":"page"},{"location":"fixedPointSolvers/#Functions","page":"Fixed Point Solvers","title":"Functions","text":"","category":"section"},{"location":"fixedPointSolvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"jacobiSolver(\n  p::QuadTeamProblem,\n  m::Int,\n  Y::Vector{<:Vector},\n  Q::Matrix{<:Vector},\n  R::Vector{<:Vector},\n  kernels::Vector{<:Function},\n  λ::Vector{Float64};\n  iterations = 5,\n  random_init = false,\n )\ngaussSeidelSolver(\n  p::QuadTeamProblem,\n  m::Int,\n  Y::Vector{<:Vector},\n  Q::Matrix{<:Vector},\n  R::Vector{<:Vector},\n  kernels::Vector{<:Function},\n  λ::Vector{Float64};\n  iterations = 5,\n  random_init = false,\n )\nSORSolver(\n  p::QuadTeamProblem,\n  m::Int,\n  Y::Vector{<:Vector},\n  Q::Matrix{<:Vector},\n  R::Vector{<:Vector},\n  kernels::Vector{<:Function},\n  λ::Vector{Float64};\n  iterations = 5,\n  random_init = false,\n  omega = 1.0,\n )","category":"page"},{"location":"fixedPointSolvers/#QuadraticTeamDecisionSolver.jacobiSolver-Tuple{QuadTeamProblem, Int64, Vector{<:Vector}, Matrix{<:Vector}, Vector{<:Vector}, Vector{<:Function}, Vector{Float64}}","page":"Fixed Point Solvers","title":"QuadraticTeamDecisionSolver.jacobiSolver","text":"jacobiSolver(\n\tp::QuadTeamProblem,\n\tm::Int,\n\tY::Vector{<:Vector},\n\tQ::Matrix{<:Vector},\n\tR::Vector{<:Vector},\n\tkernels::Vector{<:Function},\n\tλ::Vector{Float64};\n\titerations = 5,\n\trandom_init = false,`\n)\n\nImplements the Jacobi operator splitting method to generate policy updates.\n\nConverges to the unique team optimal gamma^* whenever\n\nlambda_mathrmmin   varrho(mathbfT)\n\n\n\n\n\n","category":"method"},{"location":"fixedPointSolvers/#QuadraticTeamDecisionSolver.gaussSeidelSolver-Tuple{QuadTeamProblem, Int64, Vector{<:Vector}, Matrix{<:Vector}, Vector{<:Vector}, Vector{<:Function}, Vector{Float64}}","page":"Fixed Point Solvers","title":"QuadraticTeamDecisionSolver.gaussSeidelSolver","text":"gaussSeidelSolver(\n\tp::QuadTeamProblem,\n\tm::Int,\n\tY::Vector{<:Vector},\n\tQ::Matrix{<:Vector},\n\tR::Vector{<:Vector},\n\tkernels::Vector{<:Function},\n\tλ::Vector{Float64};\n\titerations = 5,\n\trandom_init = false,\n)\n\nImplements the Gauss-Seidel operator splitting method to generate policy updates.\n\nGuaranteed to converge to the unique team optimal gamma^*.\n\n\n\n\n\n","category":"method"},{"location":"fixedPointSolvers/#QuadraticTeamDecisionSolver.SORSolver-Tuple{QuadTeamProblem, Int64, Vector{<:Vector}, Matrix{<:Vector}, Vector{<:Vector}, Vector{<:Function}, Vector{Float64}}","page":"Fixed Point Solvers","title":"QuadraticTeamDecisionSolver.SORSolver","text":"SORSolver(\n\tp::QuadTeamProblem,\n\tm::Int,\n\tY::Vector{<:Vector},\n\tQ::Matrix{<:Vector},\n\tR::Vector{<:Vector},\n\tkernels::Vector{<:Function},\n\tλ::Vector{Float64};\n\titerations = 5,\n\trandom_init = false,\n\tomega = 1.0,\n)\n\nImplements the Successive-Over-Relaxation (SOR) operator splitting method to generate policy updates.\n\nConverges to the unique team optimal gamma^* for all omega neq 0 that fulfill \n\n\t1 - fraclambda_mathrmminvarrho(mathbfT)  omega  1 + fraclambda_mathrmminvarrho(mathbfT)\n\n\n\n\n\n","category":"method"},{"location":"kernelMethods/#Scalar-and-Vector-Kernel-Methods","page":"Scalar and Vector Kernel Methods","title":"Scalar and Vector Kernel Methods","text":"","category":"section"},{"location":"kernelMethods/","page":"Scalar and Vector Kernel Methods","title":"Scalar and Vector Kernel Methods","text":"The solvers implemented in this library reproducing kernel Hilbert space methods to approximate the conditional expectation functions that arise in our iterations.","category":"page"},{"location":"kernelMethods/","page":"Scalar and Vector Kernel Methods","title":"Scalar and Vector Kernel Methods","text":"For this reason, we provide a flexible and easy to use interface that can be employed to solve both scalar and vector valued regression problems.","category":"page"},{"location":"kernelMethods/","page":"Scalar and Vector Kernel Methods","title":"Scalar and Vector Kernel Methods","text":"We only provide two kernel definitions, one for the scalar case and one for the vector case, respectively. Nevertheless, the user can easily define their own kernels and use them. We make no requirement on the type and behaviour of a kernel, other that it must be a function.","category":"page"},{"location":"kernelMethods/","page":"Scalar and Vector Kernel Methods","title":"Scalar and Vector Kernel Methods","text":"For more details on the general theory of scalar reproducing kernel Hilbert space methods, please refer to (Berlinet and Thomas-Agnan, 2004).","category":"page"},{"location":"kernelMethods/","page":"Scalar and Vector Kernel Methods","title":"Scalar and Vector Kernel Methods","text":"For more details on the formulation of vector reproducing kernel Hilbert space methods, please refer to (Micchelli and Pontil, 2005), for details on approximation properties and universality of these spaces refer to (Carmeli et al., 2008).","category":"page"},{"location":"kernelMethods/#Functions","page":"Scalar and Vector Kernel Methods","title":"Functions","text":"","category":"section"},{"location":"kernelMethods/","page":"Scalar and Vector Kernel Methods","title":"Scalar and Vector Kernel Methods","text":"exponentialKernel(x::Vector, y::Vector; h = 1)\nmatrixExponentialKernel(h::Vector{Float64}, M::Vector{<:Matrix}, λ::Vector{Float64}, x::Any, t::Any)\ngramian(kernel::Function, Y::Vector{<:Vector})\nkernelNorm(weights::Vector, kernelGramian::Matrix)\nkernelFunction(\n  kernel::Function,\n  weights::Vector,\n  X::Vector{<:Vector},\n  x::Vector,\n )\nkernelRegression(\n  kernel::Function,\n  Y::Vector,\n  X::Vector;\n  λ = 0.5,\n )","category":"page"},{"location":"kernelMethods/#QuadraticTeamDecisionSolver.exponentialKernel-Tuple{Vector, Vector}","page":"Scalar and Vector Kernel Methods","title":"QuadraticTeamDecisionSolver.exponentialKernel","text":"exponentialKernel(x::Vector, y::Vector; h = 1)\n\nRadial Basis Function Kernel \n\nArguments:\n\nx::Vector: A data sample.\ny::Vector: Also a data sample.\nh=1: Width of the kernel.\n\nReturns:\n\nValue of the kernel computed at x and y with window width h, \n\nthat is, exp left( - fracx - y^2h right) where   cdot   is the norm  on the original sample space.\n\n\n\n\n\n","category":"method"},{"location":"kernelMethods/#QuadraticTeamDecisionSolver.matrixExponentialKernel-Tuple{Vector{Float64}, Vector{<:Matrix}, Vector{Float64}, Any, Any}","page":"Scalar and Vector Kernel Methods","title":"QuadraticTeamDecisionSolver.matrixExponentialKernel","text":"matrixExponentialKernel(h::Vector{Float64}, M::Vector{<:Matrix}, λ::Vector{Float64}, x::Any, t::Any)\n\nMatrix Valued Gaussian Mixture Kernel \n\nArguments:\n\nh::Vector{Float64}: Vector of kernel bandwidths sigma_i i in  1 dots L .\nM::Vector{<:Matrix}: Vector of mixture matrices M_i i in  1 dots L .\nλ::Vector{Float64}: Vector of convex combination coefficients lambda_ii in  1 dots L ^2  sum_i=1^Llambda_i = 1.\nx::Vector: A data sample x in mathcalX.\nx::Vector: Also a data sample.\n\nReturns:\n\nValue of matrix kernel computed at x and y with specified input parameters.\n\n\tK(xt) = sum_i=1^L lambda_ie^-sigma_i^-1d_mathcalX(xy)^2M_i\n\n\n\n\n\n","category":"method"},{"location":"kernelMethods/#QuadraticTeamDecisionSolver.gramian-Tuple{Function, Vector{<:Vector}}","page":"Scalar and Vector Kernel Methods","title":"QuadraticTeamDecisionSolver.gramian","text":"gramian(kernel::Function, X::Vector{<:Vector})\n\nCompute the gramian of a kernel K over the samples mathbfX^n subset mathcalX.\n\nArguments:\n\nkernel::Function: A scalar or matrix kernel function.\nX::Vector{<:Vector}: The samples over which the gramian is to be constructed.\n\n\n\n\n\n","category":"method"},{"location":"kernelMethods/#QuadraticTeamDecisionSolver.kernelNorm-Tuple{Vector, Matrix}","page":"Scalar and Vector Kernel Methods","title":"QuadraticTeamDecisionSolver.kernelNorm","text":"kernelNorm(weights::Vector, kernelGramian::Matrix)\n\nCompute the function norm: sum_i=1^n sum_j=1^n mu_i mu_j K( x^(i) x^(j) ) of a function in a reproducing kernel Hilbert space. Th\n\nArguments:\n\nweights::Vector: The mu's in the above expression.\nkernelGramian::Matrix: The gramian of K over the samples mathbfX^n\n\n\n\n\n\n","category":"method"},{"location":"kernelMethods/#QuadraticTeamDecisionSolver.kernelFunction-Tuple{Function, Vector, Vector{<:Vector}, Vector}","page":"Scalar and Vector Kernel Methods","title":"QuadraticTeamDecisionSolver.kernelFunction","text":"kernelFunction(\n\tkernel::Function,\n\tweights::Vector,\n\tX::Vector{<:Vector},\n\tx::Vector,\n)\n\nEvaluates a kernel function of the form f(x) = sum_i=1^n mu_i K(x x^(i))\n\nArguments:\n\nkernel::Function scalar or matrix valued Kernel function that defines the RKHS where f lives.\nweights: The mu's in the above expression.\nx: The point mathbfx in mathcalX at which f is to be evaluated.\n\n\n\n\n\n","category":"method"},{"location":"kernelMethods/#QuadraticTeamDecisionSolver.kernelRegression-Tuple{Function, Vector, Vector}","page":"Scalar and Vector Kernel Methods","title":"QuadraticTeamDecisionSolver.kernelRegression","text":"kernelRegression(\n\tkernel::Function,\n\tY::Vector,\n\tX::Vector;\n\tλ = 0.5,\n)\n\nSolves the kernel regression problem\n\nf^* = argmin_f in mathcalH_k sum_i=1^n lvert lvert f(x^(i)) - y^(i) rvert rvert_mathcalY^2 + lambda lvert lvert f rvert rvert_mathcalH_k^2 \n\nWhere x^(i) y^(i) _i=1^n subset mathcalXtimesmathcalY are data samples, and mathcalH_k is a Reproducing Kernel Hilbert Space with kernel k.\n\nMoreover, this implementation can handle the case mathcalY = mathbbC^d for d  1. That is, this implements vector valued kernel regression, as well as scalar valued kernel regression.\n\nArguments:\n\nkernel::Function: The kernel function that corresponds to mathcalH_k.\nX::Vector: Vector of samples in input space mathcalX.\nY::Vector: Vector of samples in output space mathcalY.\nλ = 0.5: Regularization constant.\n\nNote that Y::Vector and X::Vector must have the same length n.\n\n\n\n\n\n","category":"method"},{"location":"#QuadraticTeamDecisionSolver.jl-Documentation","page":"Home","title":"QuadraticTeamDecisionSolver.jl Documentation","text":"","category":"section"},{"location":"#Theoretical-Problem-Description","page":"Home","title":"Theoretical Problem Description","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The quadratic team decision problem aims to optimize a team of agents' actions to minimize a quadratic objective function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Our exact formulation can be found in (Yüksel and Başar, 2013), however the problem was initially formulated in (Radner, 1962).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Importantly, in any static team decision problem, every random variable at play depends on the state of the world mathbfX, which is the variable that adds \"randomness\" to the system and thus cannot be influenced by the agents' actions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"With this in mind, the static quadratic team decision problem can be formulated as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"argmin_gamma in Gamma^1 times dots times Gamma^N J(gamma) = mathbbE left sum_ij = 1^N gamma^i(mathbfY_i)^mathsfH mathbfQ_ij gamma^j (mathbfY_j) + 2 mathrmRe left( sum_i = 1^N gamma^j (mathbfY_j)^mathsfH mathbfR_i  right) + mathbfc right","category":"page"},{"location":"","page":"Home","title":"Home","text":"where:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfY_i is the random variable the agent i measures. It has realizations in mathbbF^m_i.\nmathbfQ_ij is a random matrix that captures the quadratic term for the interaction between agent i and agent j, which depends on the state mathbfX. It has realizations in mathcalL(mathbbF^a_j mathbbF^a_i).\nmathbfR_i is a random vector that depends on the state mathbfX.\nmathbfc is a scalar valued random variable that depends on mathbfX.\ngamma_i is the policy function for agent i that maps a realization of mathbfY_i to the action mathbfU_i in mathbbF^a_i taken by agent i. In this setting, we require gamma^i to have bounded second moment with respect to mathbfY_i.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that every random variable previously mentioned is taken as a deterministic function of the state of the world mathbfX.","category":"page"},{"location":"","page":"Home","title":"Home","text":"NFurthermore,the block matrix","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfQ = beginbmatrix mathbfQ_11  dots  mathbfQ_1N \n                              vdots  ddots  vdots \n                              mathbfQ_N1  dots  mathbfQ_NN\n                              endbmatrix","category":"page"},{"location":"","page":"Home","title":"Home","text":"fulfills the following critical conditions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfQ is uniformly bounded above, that is, there exists 0  lambda_mathrmmax  infty such that","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbbPleft( lambda_mathrmmaxI - mathbfQ succ 0 right) = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfQ is uniformly bounded below, that is, there exists lambda_mathrmmin  0 such that","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbbPleft( mathbfQ - lambda_mathrmminI succ 0 right) = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"These conditions ensure a solution to our problem exists and is unique. Furthermore, they determine the convergence of the policy update schemes that we employ in our numerics.","category":"page"},{"location":"#Data-Structures-Employed","page":"Home","title":"Data Structures Employed","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We employ nested vectors for simplicity.","category":"page"},{"location":"#Measurement-Vectors-\\mathbf{Y}","page":"Home","title":"Measurement Vectors mathbfY","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We store the samples of our measurement vectors as a vector{<:vector}. In particular, if we have m samples, then Y is a vector of length N (number of agents) of vectors of length m. Each m length vector then contains m vectors, each of length m_i (dimension of measurements for agent i) of real or complex numbers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example, suppose we have 3 agents with m_1 = 1 m_2 = 2 and m_3 = 3 and we have 10 samples. Then Y is a vector of length 3 with","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfY1 = left lefty1_1^(1) right dots  lefty1_1^(10)right right","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfY2 = left lefty2_1^(1) y2_2^(1)right dots  lefty2_1^(10) y2_2^(10)right right","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfY3 = left lefty3_1^(1) y3_2^(1) y3_2^(1)right dots  lefty3_1^(10) y3_2^(10) y3_2^(10)right right","category":"page"},{"location":"#System-Random-Vectors-\\mathbf{R}","page":"Home","title":"System Random Vectors mathbfR","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"These employ the same exact structure as the measurement vectors mathbfY.","category":"page"},{"location":"#System-Random-Matrices-\\mathbf{Q}","page":"Home","title":"System Random Matrices mathbfQ","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In this case, we use the same basic structure as before, but double indexed. In particular, if we have m samples, then Q is an N by N matrix of vectors of length m. The vector in the ij-th position then contains m matrices, each of dimensions a_j times a_i (dimension of measurements for agent i) of real or complex numbers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example, suppose we have 2 agents with a_1 = 1 and a_2 = 2 and we have 10 samples. Then Q is a 2 by 2 matrix with","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfQ11 = left leftQ11_1^(1) right dots  leftQ11_1^(10)right right","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfQ12 = left beginbmatrix Q12_1^(1)  Q12_2^(1) endbmatrix  dots  beginbmatrix Q12_1^(10)  Q12_2^(10) endbmatrix right","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfQ21 = left beginbmatrix Q21_1^(1)  Q21_2^(1) endbmatrix  dots  beginbmatrix Q21_1^(10)  Q21_2^(10) endbmatrix right","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfQ22 = left beginbmatrix Q22_11^(1)  Q22_12^(1)  Q22_21^(1)  Q22_22^(1) endbmatrix  dots  beginbmatrix Q22_11^(10)  Q22_12^(10)  Q22_21^(10)  Q22_22^(10) endbmatrix right","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Berlinet, A. and Thomas-Agnan, C. (2004). Reproducing Kernel Hilbert Spaces in Probability and Statistics. 1 Edition, Springer Book Archive (Springer, New York, NY); p. XXII, 355. Published: 31 December 2004.\n\n\n\nCarmeli, C.; Vito, E. D.; Toigo, A. and Umanità, V. (2008). Vector valued reproducing kernel Hilbert spaces and universality, arXiv preprint 0807.1659.\n\n\n\nMicchelli, C. A. and Pontil, M. (2005). On Learning Vector-Valued Functions. Neural Computation 17, 177–204.\n\n\n\nMiretti, L.; Bjornson, E. and Gesbert, D. (oct 2021). Team Precoding Towards Scalable Cell-free Massive MIMO Networks. In: 2021 55th Asilomar Conference on Signals, Systems, and Computers (IEEE).\n\n\n\nMiretti, L.; Cavalcante, R. L. and Stanczak, S. (2022). Joint optimal beamforming and power control in cell-free massive MIMO, arXiv preprint 2208.01385.\n\n\n\nde Pillis, J. (1973). Gauss–Seidel Convergence for Operators on Hilbert Space. SIAM Journal on Numerical Analysis 10, 112–122, arXiv: https://doi.org/10.1137/0710012.\n\n\n\nRadner, R. (1962). Team Decision Problems. The Annals of Mathematical Statistics 33, 857–881.\n\n\n\nYüksel, S. and Başar, T. (2013). Stochastic Networked Control Systems. 2 Edition (Birkhäuser New York, NY).\n\n\n\n","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
